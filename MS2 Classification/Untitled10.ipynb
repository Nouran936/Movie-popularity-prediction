{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV,\\\n",
    "RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer, FunctionTransformer, PolynomialFeatures, \\\n",
    "    LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, precision_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectKBest , f_classif\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Select(df,Y):\n",
    "    from sklearn.feature_selection import mutual_info_classif\n",
    "    import matplotlib.pyplot as plt\n",
    "    df = df.fillna(0)\n",
    "    imp = mutual_info_classif(df, y_train)\n",
    "    # Create a series of feature importances and plot them\n",
    "    feat = pd.Series(imp, df.columns)\n",
    "    feat.plot(kind='barh', color='teal')\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "def NumericPreProcessing(df):\n",
    "    # removing zeros\n",
    "    df[['budget', 'viewercount', 'revenue', 'runtime', 'vote_count', 'release_date']] \\\n",
    "        = df[['budget', 'viewercount', 'revenue', 'runtime', 'vote_count', 'release_date']] \\\n",
    "        .replace(0, df[['budget', 'viewercount', 'revenue', 'runtime', 'vote_count', 'release_date']].mean())\n",
    "\n",
    "    # scaling\n",
    "    cols_to_scale = ['viewercount', 'runtime', 'vote_count', 'budget', 'revenue', 'release_date']\n",
    "    df[cols_to_scale] = (df[cols_to_scale] - df[cols_to_scale].min()) / (\n",
    "                df[cols_to_scale].max() - df[cols_to_scale].min())\n",
    "    cols = df[cols_to_scale]\n",
    "\n",
    "    return cols\n",
    "\n",
    "\n",
    "def colsToNormalize(df):\n",
    "    data = pd.DataFrame()\n",
    "    df['homepage'] = df['homepage'].notnull().astype(int)\n",
    "    data = pd.concat([data, df['homepage']], axis=1)\n",
    "\n",
    "     #label encoding\n",
    "    df['status'] = df['status'].notnull().astype(int)\n",
    "    data = pd.concat([data, df['status']], axis = 1)\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df['original_language'] = le.fit_transform(df['original_language'])\n",
    "    df['original_language'] = le.fit_transform(df.original_language.values)\n",
    "\n",
    "    data = pd.concat([data, df['original_language']], axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        text = ''\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Perform lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Join the list of tokens into a single string\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "mlb_key = MultiLabelBinarizer()\n",
    "mlbgen = MultiLabelBinarizer()\n",
    "mlbpc = MultiLabelBinarizer()\n",
    "mlb2 = MultiLabelBinarizer()\n",
    "mlbsp = MultiLabelBinarizer()\n",
    "\n",
    "\n",
    "def dictionaryPreprocessing(df):\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # Convert the 'genres' column to a list of genre names\n",
    "    df['genres'] = df['genres'].apply(lambda x: ','.join([d['name'] for d in eval(x)]))\n",
    "\n",
    "    df['keywords'] = df['keywords'].apply(lambda x: ','.join([d['name'] for d in eval(x)]))\n",
    "\n",
    "    df['production_companies'] = df['production_companies'].apply(lambda x: ','.join([d['name'] for d in eval(x)]))\n",
    "\n",
    "    # Convert the 'production_countries' column to a list of country names\n",
    "    df['production_countries'] = df['production_countries'].apply(\n",
    "        lambda x: ','.join([d['iso_3166_1'] for d in eval(x)]))\n",
    "\n",
    "    # Convert the 'spoken_languages' column to a list of language names\n",
    "    df['spoken_languages'] = df['spoken_languages'].apply(lambda x: ','.join([d['name'] for d in eval(x)]))\n",
    "\n",
    "    mlb_key.fit(df['keywords'])\n",
    "    transkey = mlb_key.transform(df['keywords'])\n",
    "    transskey = pd.DataFrame(transkey)\n",
    "    data = pd.concat([data, transskey], axis=1)\n",
    "\n",
    "    mlbgen.fit(df['genres'])\n",
    "    transgen = mlbgen.transform(df['genres'])\n",
    "    transsgen = pd.DataFrame(transgen)\n",
    "    data = pd.concat([data, transsgen], axis=1)\n",
    "\n",
    "    mlbpc.fit(df['production_countries'])\n",
    "    transpc = mlbpc.transform(df['production_countries'])\n",
    "    transspc = pd.DataFrame(transpc)\n",
    "    data = pd.concat([data, transspc], axis=1)\n",
    "\n",
    "    mlbsp.fit(df['spoken_languages'])\n",
    "    transp = mlbsp.transform(df['spoken_languages'])\n",
    "    transsp = pd.DataFrame(transp)\n",
    "    data = pd.concat([data, transsp], axis=1)\n",
    "    #\n",
    "\n",
    "    mlb2.fit(df['production_companies'])\n",
    "    trans2 = mlb2.transform(df['production_companies'])\n",
    "    transs2 = pd.DataFrame(trans2)\n",
    "    data = pd.concat([data, transs2], axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def dictionaryPreprocessing_test(df):\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # Convert the 'genres' column to a list of genre names\n",
    "    df['genres'] = df['genres'].apply(lambda x: ','.join([d['name'] for d in eval(x)]))\n",
    "\n",
    "    df['keywords'] = df['keywords'].apply(lambda x: ','.join([d['name'] for d in eval(x)]))\n",
    "\n",
    "    df['production_companies'] = df['production_companies'].apply(lambda x: ','.join([d['name'] for d in eval(x)]))\n",
    "\n",
    "    # Convert the 'production_countries' column to a list of country names\n",
    "    df['production_countries'] = df['production_countries'].apply(\n",
    "        lambda x: ','.join([d['iso_3166_1'] for d in eval(x)]))\n",
    "\n",
    "    # Convert the 'spoken_languages' column to a list of language names\n",
    "    df['spoken_languages'] = df['spoken_languages'].apply(lambda x: ','.join([d['name'] for d in eval(x)]))\n",
    "\n",
    "    transkey = mlb_key.transform(df['keywords'])\n",
    "    transskey = pd.DataFrame(transkey)\n",
    "    data = pd.concat([data, transskey], axis=1)\n",
    "\n",
    "    transgen = mlbgen.transform(df['genres'])\n",
    "    transsgen = pd.DataFrame(transgen)\n",
    "    data = pd.concat([data, transsgen], axis=1)\n",
    "\n",
    "    transpc = mlbpc.transform(df['production_countries'])\n",
    "    transspc = pd.DataFrame(transpc)\n",
    "    data = pd.concat([data, transspc], axis=1)\n",
    "\n",
    "    transp = mlbsp.transform(df['spoken_languages'])\n",
    "    transsp = pd.DataFrame(transp)\n",
    "    data = pd.concat([data, transsp], axis=1)\n",
    "\n",
    "    trans2 = mlb2.transform(df['production_companies'])\n",
    "    transs2 = pd.DataFrame(trans2)\n",
    "    data = pd.concat([data, transs2], axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "df = pd.read_csv('C:/Users/Nouran/OneDrive/Desktop/movies-classification-dataset.csv', parse_dates=['release_date'],\n",
    "                 dayfirst=True, engine='python')\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "df['release_date'] = pd.to_datetime(df['release_date'])\n",
    "reference_date = datetime.now()\n",
    "df['release_date'] = (reference_date - df['release_date']).dt.days\n",
    "#vectorizer = TfidfVectorizer()\n",
    "X_train= df.iloc[:, :-1]\n",
    "y_train= df['Rate']\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, shuffle=False, random_state=1000)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('C:/Users/Nouran/OneDrive/Desktop/day1/M2/movies-tas-test.csv', parse_dates=['release_date'],\n",
    "                 dayfirst=True, engine='python')\n",
    "df_test.drop_duplicates(inplace=True)\n",
    "\n",
    "df_test['release_date'] = pd.to_datetime(df_test['release_date'])\n",
    "reference_date = datetime.now()\n",
    "df_test['release_date'] = (reference_date - df_test['release_date']).dt.days\n",
    "X_test= df.iloc[:, :-1]\n",
    "y_test= df['Rate']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm\n",
      "(3025, 3)\n",
      "num\n",
      "(3025, 6)\n",
      "fea\n",
      "dict\n",
      "(3025, 114)\n",
      "nlp\n",
      "(3025, 16373)\n",
      "norm\n",
      "(3025, 3)\n",
      "num\n",
      "(3025, 6)\n",
      "dict\n",
      "(3025, 114)\n",
      "nlp\n",
      "(3025, 16373)\n",
      "Loading Trained Model\n",
      "SVM accuracy:  0.8135537190082645\n",
      "Loading Trained Model\n",
      "accuracy:  0.8671074380165289\n",
      "Loading Trained Model\n",
      "Decision Tree Accuracy: 0.6330578512396694\n"
     ]
    }
   ],
   "source": [
    "X_train['overview'] = X_train['overview'].apply(preprocess_text)\n",
    "X_test['overview'] = X_test['overview'].apply(preprocess_text)\n",
    "\n",
    "X_train['title']= X_train['title'].apply(preprocess_text)\n",
    "X_test['title']=  X_test['title'].apply(preprocess_text)\n",
    "\n",
    "X_train['tagline']= X_train['tagline'].apply(preprocess_text)\n",
    "X_test['tagline']=  X_test['tagline'].apply(preprocess_text)\n",
    "\n",
    "X_train['original_title']= X_train['original_title'].apply(preprocess_text)\n",
    "X_test['original_title']=  X_test['original_title'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tag_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tagtrain = Tag_vectorizer.fit_transform(X_train['tagline'])\n",
    "tag_train= pd.DataFrame(tagtrain.toarray(),columns=Tag_vectorizer.get_feature_names_out())\n",
    "\n",
    "tag_test = Tag_vectorizer.transform(X_test['tagline'])\n",
    "tag_test= pd.DataFrame(tag_test.toarray(),columns=Tag_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OTitle_vectorizer = TfidfVectorizer()\n",
    "Otitle_train = OTitle_vectorizer.fit_transform(X_train['original_title'])\n",
    "Otitle_train= pd.DataFrame(Otitle_train.toarray(),columns=OTitle_vectorizer.get_feature_names_out())\n",
    "Otitle_test = OTitle_vectorizer.transform(X_test['original_title'])\n",
    "Otitle_test= pd.DataFrame(Otitle_test.toarray(),columns=OTitle_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Title_vectorizer = TfidfVectorizer()\n",
    "title_train = Title_vectorizer.fit_transform(X_train['title'])\n",
    "title_train= pd.DataFrame(title_train.toarray(),columns=Title_vectorizer.get_feature_names_out())\n",
    "title_test = Title_vectorizer.transform(X_test['title'])\n",
    "title_test= pd.DataFrame(title_test.toarray(),columns=Title_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "nlp_train = vectorizer.fit_transform(X_train['overview'])\n",
    "nlp_train= pd.DataFrame(nlp_train.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "nlp_test = vectorizer.transform(X_test['overview'])\n",
    "nlp_test= pd.DataFrame(nlp_test.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "selector1 = VarianceThreshold(threshold=0.0001)\n",
    "X_train_selected_Num = selector1.fit_transform(NumericPreProcessing(X_train))\n",
    "X_test_selected_Num = selector1.transform(NumericPreProcessing(X_test))\n",
    "# print(X_train_selected.shape)\n",
    "# print(X_test_selected.shape)\n",
    "X_train_selected_Num = pd.DataFrame(X_train_selected_Num)\n",
    "X_test_selected_Num = pd.DataFrame(X_test_selected_Num)\n",
    "##\n",
    "\n",
    "dataAbdallah = pd.DataFrame()\n",
    "dataAbdallah = pd.concat([dataAbdallah, colsToNormalize(X_train)], axis=1)\n",
    "print(\"norm\")\n",
    "print(dataAbdallah.shape)\n",
    "dataAbdallah = dataAbdallah.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dataAbdallah = pd.concat([dataAbdallah, X_train_selected_Num], axis=1)\n",
    "\n",
    "print(\"num\")\n",
    "print(dataAbdallah.shape)\n",
    "# dataAbdallah = dataAbdallah.reset_index(drop=True)\n",
    "### fea\n",
    "print(\"fea\")\n",
    "selector = VarianceThreshold(threshold=0.08)\n",
    "X_train_selected = selector.fit_transform(dictionaryPreprocessing(X_train))\n",
    "X_test_selected = selector.transform(dictionaryPreprocessing_test(X_test))\n",
    "# print(X_train_selected.shape)\n",
    "# print(X_test_selected.shape)\n",
    "X_train_selected = pd.DataFrame(X_train_selected)\n",
    "X_test_selected = pd.DataFrame(X_test_selected)\n",
    "####\n",
    "\n",
    "###\n",
    "dataAbdallah = pd.concat([dataAbdallah, X_train_selected], axis=1)\n",
    "print(\"dict\")\n",
    "print(dataAbdallah.shape)\n",
    "\n",
    "dataAbdallah = pd.concat([dataAbdallah, nlp_train], axis=1)\n",
    "print(\"nlp\")\n",
    "print(dataAbdallah.shape)\n",
    "\n",
    "dataAbdallah = dataAbdallah.fillna(0)\n",
    "################################\n",
    "\n",
    "\n",
    "dataNaguib = pd.DataFrame()\n",
    "dataNaguib = pd.concat([dataNaguib, colsToNormalize(X_test)], axis=1)\n",
    "print(\"norm\")\n",
    "print(dataNaguib.shape)\n",
    "dataNaguib = dataNaguib.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dataNaguib = pd.concat([dataNaguib, X_test_selected_Num], axis=1)\n",
    "print(\"num\")\n",
    "print(dataNaguib.shape)\n",
    "dataNaguib = dataNaguib.reset_index(drop=True)\n",
    "\n",
    "dataNaguib = pd.concat([dataNaguib, X_test_selected], axis=1)\n",
    "print(\"dict\")\n",
    "print(dataNaguib.shape)\n",
    "\n",
    "dataNaguib = pd.concat([dataNaguib, nlp_test], axis=1)\n",
    "print(\"nlp\")\n",
    "print(dataNaguib.shape)\n",
    "\n",
    "dataNaguib = dataNaguib.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataAbdallah.columns = dataAbdallah.columns.astype(str)\n",
    "dataNaguib.columns = dataNaguib.columns.astype(str)\n",
    "\n",
    "\n",
    "def svm_model():\n",
    "    filename = 'SVM_model.sav'\n",
    "    if os.path.exists('SVM_model.sav'):\n",
    "        print(\"Loading Trained Model\")\n",
    "        loaded_model = pickle.load(open(filename, 'rb'))\n",
    "        prediction = loaded_model.predict(dataNaguib)\n",
    "        print(\"SVM accuracy: \", accuracy_score(y_test, prediction))\n",
    "    else:\n",
    "        print(\"Creating and training a new model\")\n",
    "        print(\"Model training started\")\n",
    "        print(\"SVM\")\n",
    "        svc_model = LinearSVC(C=0.1, max_iter=1000)\n",
    "        start_train = time.time()\n",
    "        svc_model.fit(dataAbdallah, y_train)\n",
    "        end_train = time.time()\n",
    "        # Calculate training time\n",
    "        training_time = end_train - start_train\n",
    "        pickle.dump(svc_model, open(filename, 'wb'))\n",
    "        start_test = time.time()\n",
    "        prediction = svc_model.predict(dataNaguib)\n",
    "        end_test = time.time()\n",
    "        # Calculate testing time\n",
    "        testing_time = end_test - start_test\n",
    "        print(\"trianing time:\", training_time)\n",
    "        print(\"test time:\", testing_time)\n",
    "        print(\"SVM accuracy: \", accuracy_score(y_test, prediction))\n",
    "\n",
    "        # Create 3 bar graphs\n",
    "        fig, plot = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "        # Bar graph for total training time\n",
    "        plot[0].bar(0, training_time)\n",
    "        plot[0].set_xticks([])\n",
    "        plot[0].set_title(\"Total Training Time(sec)\")\n",
    "\n",
    "        # Bar graph for total testing time\n",
    "        plot[1].bar(0, testing_time)\n",
    "        plot[1].set_xticks([])\n",
    "        plot[1].set_title(\"Total Testing Time(sec)\")\n",
    "\n",
    "        # Bar graph for SMV accuracy\n",
    "        plot[2].bar(0, accuracy_score(y_test, prediction) * 100)\n",
    "        plot[2].set_xticks([])\n",
    "        plot[2].set_title(\"SVM accuracy(%)\")\n",
    "\n",
    "\n",
    "def log_model():\n",
    "    filename = 'LOG_model.sav'\n",
    "    if os.path.exists('LOG_model.sav'):\n",
    "        print(\"Loading Trained Model\")\n",
    "        # load the model from disk\n",
    "        loaded_model = pickle.load(open(filename, 'rb'))\n",
    "        y_pred = loaded_model.predict(dataNaguib)\n",
    "        print('accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    else:\n",
    "        print(\"Creating and training a new model\")\n",
    "        print(\"Model training started\")\n",
    "        print(\"LOGISTIC\")\n",
    "        # lr = LogisticRegression(penalty='l2', solver='sag', max_iter=1000, C=0.01)\n",
    "        lr = LogisticRegression(penalty='l2', solver='saga', max_iter=1000, C=1.0)\n",
    "        # lr = LogisticRegression(penalty='l2', solver='saga', max_iter=1000, C=0.5)\n",
    "        # lr = LogisticRegression(penalty='l2', solver='newton-cg', max_iter=1000, C=0.01)\n",
    "        # lr = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000, C=0.1)\n",
    "        start_train = time.time()\n",
    "        lr.fit(dataAbdallah, y_train)\n",
    "        end_train = time.time()\n",
    "        # Calculate training time\n",
    "        training_time = end_train - start_train\n",
    "        pickle.dump(lr, open(filename, 'wb'))\n",
    "        start_test = time.time()\n",
    "        y_pred = lr.predict(dataNaguib)\n",
    "        end_test = time.time()\n",
    "        # Calculate testing time\n",
    "        testing_time = end_test - start_test\n",
    "\n",
    "        print(\"trianing time:\", training_time)\n",
    "        print(\"test time:\", testing_time)\n",
    "        print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        # Create 3 bar graphs\n",
    "        fig, plot = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "        # Bar graph for total training time\n",
    "        plot[0].bar(0, training_time)\n",
    "        plot[0].set_xticks([])\n",
    "        plot[0].set_title(\"Total Training Time(sec)\")\n",
    "\n",
    "        # Bar graph for total test time\n",
    "        plot[1].bar(0, testing_time)\n",
    "        plot[1].set_xticks([])\n",
    "        plot[1].set_title(\"Total Test Time(sec)\")\n",
    "\n",
    "        # Bar graph for logistic regression accuracy\n",
    "        plot[2].bar(0, metrics.accuracy_score(y_test, y_pred) * 100)\n",
    "        plot[2].set_xticks([])\n",
    "        plot[2].set_title(\"Logistic reg accuracy(%)\")\n",
    "\n",
    "\n",
    "def tree_model():\n",
    "    filename = 'TREE_model.sav'\n",
    "    if os.path.exists('TREE_model.sav'):\n",
    "        print(\"Loading Trained Model\")\n",
    "        # load the model from disk\n",
    "        loaded_model = pickle.load(open(filename, 'rb'))\n",
    "        y_pred = loaded_model.predict(dataNaguib)\n",
    "        print(\"Decision Tree Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "    else:\n",
    "        print(\"Creating and training a new model\")\n",
    "        print(\"Model training started\")\n",
    "        print(\"Decison Tree\")\n",
    "        # clf = DecisionTreeClassifier()\n",
    "        # clf = DecisionTreeClassifier(criterion = \"entropy\")\n",
    "        clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=2)\n",
    "        start_train = time.time()\n",
    "        clf = clf.fit(dataAbdallah, y_train)\n",
    "        end_train = time.time()\n",
    "        # Calculate training time\n",
    "        training_time = end_train - start_train\n",
    "        pickle.dump(clf, open(filename, 'wb'))\n",
    "        start_test = time.time()\n",
    "        y_pred = clf.predict(dataNaguib)\n",
    "        end_test = time.time()\n",
    "        # Calculate testing time\n",
    "        testing_time = end_test - start_test\n",
    "\n",
    "        print(\"trianing time:\", training_time)\n",
    "        print(\"test time:\", testing_time)\n",
    "        print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "        # Create 3 bar graphs\n",
    "        fig, plot = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "        # Bar graph for total training time\n",
    "        plot[0].bar(0, training_time)\n",
    "        plot[0].set_xticks([])\n",
    "        plot[0].set_title(\"Total Training Time(sec)\")\n",
    "\n",
    "        # Bar graph for total test time\n",
    "        plot[1].bar(0, testing_time)\n",
    "        plot[1].set_xticks([])\n",
    "        plot[1].set_title(\"Total Test Time(sec)\")\n",
    "\n",
    "        # Bar graph for decision tree accuracy\n",
    "        plot[2].bar(0, metrics.accuracy_score(y_test, y_pred) * 100)\n",
    "        plot[2].set_xticks([])\n",
    "        plot[2].set_title(\"Decision Tree accuracy(%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "svm_model()\n",
    "\n",
    "\n",
    "log_model()\n",
    "\n",
    "\n",
    "\n",
    "tree_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
